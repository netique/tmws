[
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "PÅ™Ã­prava",
    "section": "",
    "text": "NÃ­Å¾e je seznam vÄ›cÃ­, kterÃ© v rÃ¡mci workshopu pravdÄ›podobnÄ› vyuÅ¾ijeme. Mezi nimi se â€“ velmi netradiÄnÄ› â€“ nachÃ¡zÃ­ i modul pro jazyk Python. A jelikoÅ¾ jsou workshopy â€œeRkaÅ™skÃ©â€, v sekci nÃ­Å¾e je nÃ¡vod, jak celou instalaci zvlÃ¡dnout z R konzole RStudia. KvÅ¯li nÃ¡rokÅ¯m na vÃ½poÄetnÃ­ vÃ½kon je ale tento krok spÃ­Å¡e doporuÄenÃ½1 a moÅ¾nÃ¡ bude vÃ½hodnÄ›jÅ¡Ã­ a pÅ™Ã­jemnÄ›jÅ¡Ã­ vyuÅ¾Ã­t tento Jupyter Notebook v rÃ¡mci Google Colab. K pouÅ¾Ã­vÃ¡nÃ­ (zdarma) je zapotÅ™ebÃ­ jen Google ÃºÄet."
  },
  {
    "objectID": "setup.html#interprety2-programovacÃ­ch-jazykÅ¯-a-ide",
    "href": "setup.html#interprety2-programovacÃ­ch-jazykÅ¯-a-ide",
    "title": "PÅ™Ã­prava",
    "section": "Interprety2 programovacÃ­ch jazykÅ¯ a IDE",
    "text": "Interprety2 programovacÃ­ch jazykÅ¯ a IDE\n\nR (verze 4.0.0 a novÄ›jÅ¡Ã­)\nPython (ideÃ¡lnÄ› ve verzi 3.11.63)\nRStudio Desktop4 IDE v aktuÃ¡lnÃ­ verzi"
  },
  {
    "objectID": "setup.html#balÃ­ky-do-r",
    "href": "setup.html#balÃ­ky-do-r",
    "title": "PÅ™Ã­prava",
    "section": "BalÃ­ky do R",
    "text": "BalÃ­ky do R\n\ntidyverse\nhere\ntidytext\ntopicmodels\nLDAvis\nldatuning\nreticulate (kdyÅ¾ budete chtÃ­t zkouÅ¡et Python v R)"
  },
  {
    "objectID": "setup.html#moduly-do-pythonu",
    "href": "setup.html#moduly-do-pythonu",
    "title": "PÅ™Ã­prava",
    "section": "Moduly do Pythonu",
    "text": "Moduly do Pythonu\n\nbertopic (verze 0.15.0)\n\n\nDoporuÄenÃ½ postup instalace pÅ™es R a RStudio\n\nlibrary(reticulate)\n\n# nainstalujeme utilitu pro sprÃ¡vu izolovanÃ½ch prostÅ™edÃ­ Pythonu\ninstall_miniconda()\n\n# vytvoÅ™Ã­me prostÅ™edÃ­ s nÃ¡zvem \"topic_modeling\", zÃ¡kladem bude Python 3.11.6\nconda_create(\"topic_modeling\", python_version = \"3.11.6\")\n\n# nainstalujeme do nÄ›j modul bertopic 0.15.0\nconda_install(\"topic_modeling\", \"bertopic==0.15.0\")\n\n# vyÅ¾Ã¡dÃ¡me specifickou verzi zÃ¡vislosti bertopicu, kterÃ¡ funguje...\nconda_install(\"topic_modeling\", \"transformers==4.35.2\")\n\n# pouÅ¾ijeme nastavenÃ© prostÅ™edÃ­\nuse_condaenv(\"topic_modeling\")\n\n# potvrdÃ­me si, Å¾e RStudio pracuje s danÃ½m prostÅ™edÃ­m\n# nÄ›kdy je nutnÃ© restartovat R session\npy_config()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MateriÃ¡ly k workshopu",
    "section": "",
    "text": "Zde budou postupnÄ› pÅ™ibÃ½vat materiÃ¡ly pro workshop. ZatÃ­m se podÃ­vejte, co si na svÃ©m poÄÃ­taÄi pÅ™ipravit, pokud si budete chtÃ­t vÄ›ci sami zkouÅ¡et."
  },
  {
    "objectID": "index.html#novinky",
    "href": "index.html#novinky",
    "title": "MateriÃ¡ly k workshopu",
    "section": "Novinky",
    "text": "Novinky\n\nv hlavnÃ­ nabÃ­dce jsou â€œvyrenderovanÃ©â€ analÃ½zy (zdrojovÃ½ kÃ³d je takÃ© na GitHubu) (2. 12. 2023)\nv hlavnÃ­ nabÃ­dce je k dispozici prezentace (zdrojovÃ½ kÃ³d je takÃ© na GitHubu) (2. 12. 2023)\nveÅ¡kerÃ½ kÃ³d je na GitHub repozitÃ¡Å™i (30. 11. 2023)\nv zÃ¡loÅ¾ce pÅ™iprava novÄ› najdete odkaz na Jupyter Notebok na Google Colab, kde je ukÃ¡zka topic modelingu v Pythonu pomocÃ­ modulu bertopic (29. 11. 2023)"
  },
  {
    "objectID": "analyses/ukazka_bertopic_py.html",
    "href": "analyses/ukazka_bertopic_py.html",
    "title": "UkÃ¡zka BERTopic",
    "section": "",
    "text": "from bertopic import BERTopic\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sentence_transformers import SentenceTransformer\n\n\ndocs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']\n\n\n# 384-dimensional sentence embeddings (cf. GPT davinci with 12K+)\n\nembedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"mps\")\n\n# if NOT on Apple Silicon device, use rather\n# embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n\nembeddings = embedding_model.encode(docs, show_progress_bar=True)\n\n\n\n\n\ntopic_model = BERTopic()\ntopics, probs = topic_model.fit_transform(docs, embeddings=embeddings)\n\n\ntopic_model.visualize_barchart()"
  },
  {
    "objectID": "analyses/lda.html",
    "href": "analyses/lda.html",
    "title": "Latent Dirichlet Allocation (LDA)",
    "section": "",
    "text": "library(tidyverse)\n\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” dplyr     1.1.4     âœ” readr     2.1.4\nâœ” forcats   1.0.0     âœ” stringr   1.5.0\nâœ” ggplot2   3.4.4     âœ” tibble    3.2.1\nâœ” lubridate 1.9.3     âœ” tidyr     1.3.0\nâœ” purrr     1.0.2     \nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::filter() masks stats::filter()\nâœ– dplyr::lag()    masks stats::lag()\nâ„¹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(topicmodels)\nlibrary(ldatuning)\nlibrary(tidytext)\n\ndata(\"AssociatedPress\")\n\nData jsou uloÅ¾ena jako document-term matrix, pÅ™eveÄme si je tedy do tibble.\n\nap_tidy &lt;- AssociatedPress |&gt; tidy()\n\nap_tidy\n\n# A tibble: 302,031 Ã— 3\n   document term       count\n      &lt;int&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1        1 adding         1\n 2        1 adult          2\n 3        1 ago            1\n 4        1 alcohol        1\n 5        1 allegedly      1\n 6        1 allen          1\n 7        1 apparently     2\n 8        1 appeared       1\n 9        1 arrested       1\n10        1 assault        1\n# â„¹ 302,021 more rows\n\n\nPouÅ¾ijeme seznam stop slov z tidytext a pÅ™idÃ¡me si vlastnÃ­:\n\nstop_words &lt;- stop_words |&gt; add_row(word = c(\"th\", \"sr\", \"r\", \"h\", \"e\"))\n\nA smaÅ¾eme je z naÅ¡ich dat. PotÃ© z tidy formÃ¡tu vyrobÃ­me zpÄ›t document-term matrix.\n\nap_dtm &lt;- ap_tidy |&gt;\n  anti_join(stop_words, by = c(term = \"word\")) |&gt;\n  cast_dtm(document, term, count)\n\nap_dtm\n\n&lt;&lt;DocumentTermMatrix (documents: 2246, terms: 10133)&gt;&gt;\nNon-/sparse entries: 259203/22499515\nSparsity           : 99%\nMaximal term length: 18\nWeighting          : term frequency (tf)\n\n\n\noptimal.topics &lt;- FindTopicsNumber(\n  ap_dtm,\n  topics = 2:8,\n  metrics = c(\"Griffiths2004\", \"CaoJuan2009\", \"Arun2010\", \"Deveaud2014\"),\n  control = list(seed = 123),\n  mc.cores = parallel::detectCores() / 2, # zapojenÃ­ vÅ¡ech jader zpÅ¯sobÃ­ pÃ¡d R...\n  verbose = TRUE\n)\n\nfit models... done.\ncalculate metrics:\n  Griffiths2004... done.\n  CaoJuan2009... done.\n  Arun2010... done.\n  Deveaud2014... done.\n\n\n\nFindTopicsNumber_plot(optimal.topics)\n\nWarning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\nâ„¹ The deprecated feature was likely used in the ldatuning package.\n  Please report the issue at &lt;https://github.com/nikita-moor/ldatuning/issues&gt;.\n\n\n\n\n\n\n\n\n\nDejme tomu, Å¾e 5 tÃ©mat uÅ¾ mÃ¡ dobrÃ© metriky a nenÃ­ to zas tak mocâ€¦\nâ€œFitnemeâ€ finÃ¡lnÃ­ model:\n\nap_lda &lt;- LDA(ap_dtm, k = 5, control = list(seed = 123))\n\n\n# beta = pravdÄ›podobnost, Å¾e slovo bylo \"vygenerovÃ¡no\" danÃ½m tÃ©matem\nap_topics &lt;- tidy(ap_lda, matrix = \"beta\")\n\nap_topics\n\n# A tibble: 50,665 Ã— 3\n   topic term       beta\n   &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt;\n 1     1 adding 1.46e- 4\n 2     2 adding 1.01e- 4\n 3     3 adding 1.35e- 4\n 4     4 adding 1.96e- 4\n 5     5 adding 3.15e- 4\n 6     1 adult  3.14e- 5\n 7     2 adult  1.65e-17\n 8     3 adult  9.17e-16\n 9     4 adult  3.41e- 5\n10     5 adult  2.41e- 4\n# â„¹ 50,655 more rows\n\n\n\nap_top_terms &lt;- ap_topics %&gt;%\n  group_by(topic) %&gt;%\n  slice_max(beta, n = 10) %&gt;%\n  ungroup() %&gt;%\n  arrange(topic, -beta)\n\nap_top_terms %&gt;%\n  mutate(term = reorder_within(term, beta, topic)) %&gt;%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~topic, scales = \"free\") +\n  scale_y_reordered()\n\n\n\n\n\n\n\n\nSlova, co se s nÄ›jakou pravdÄ›podobnostÃ­ vyskytujÃ­ tak nÄ›jak napÅ™Ã­Ä:\n\nap_topics |&gt;\n  filter(beta &gt; .001) |&gt;\n  pivot_wider(names_from = topic, values_from = beta, names_prefix = \"topic_\") |&gt;\n  filter(if_all(-term, \\(x) !is.na(x)))\n\n# A tibble: 11 Ã— 6\n   term      topic_4 topic_5 topic_3 topic_2 topic_1\n   &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 friday    0.00230 0.00170 0.00210 0.00107 0.00128\n 2 day       0.00146 0.00309 0.00160 0.00115 0.00174\n 3 million   0.0132  0.00140 0.00158 0.00319 0.00284\n 4 time      0.00162 0.00342 0.00190 0.00237 0.00344\n 5 wednesday 0.00162 0.00201 0.00192 0.00129 0.00194\n 6 monday    0.00167 0.00197 0.00187 0.00173 0.00139\n 7 american  0.00332 0.00192 0.00210 0.00135 0.00190\n 8 national  0.00154 0.00179 0.00231 0.00171 0.00392\n 9 tuesday   0.00216 0.00166 0.00172 0.00166 0.00219\n10 week      0.00296 0.00108 0.00170 0.00158 0.00237\n11 thursday  0.00265 0.00159 0.00201 0.00166 0.00205\n\n\nTeÄ k pravdÄ›podobnosti tÃ©mat v rÃ¡mci jednotlivÃ½ch dokumentÅ¯:\n\n# gamma = podÃ­l slov v dokumentu, kterÃ© byly \"vygenerovÃ¡ny\" danÃ½m tÃ©matem\nap_documents &lt;- tidy(ap_lda, matrix = \"gamma\")\n\nap_documents |&gt;\n  mutate(document = as.integer(document)) |&gt;\n  arrange(document, topic)\n\n# A tibble: 11,230 Ã— 3\n   document topic    gamma\n      &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;\n 1        1     1 0.000319\n 2        1     2 0.0451  \n 3        1     3 0.000319\n 4        1     4 0.000319\n 5        1     5 0.954   \n 6        2     1 0.000312\n 7        2     2 0.459   \n 8        2     3 0.442   \n 9        2     4 0.0975  \n10        2     5 0.000312\n# â„¹ 11,220 more rows\n\n\nSouÄet hodnot \\gamma pro kaÅ¾dÃ½ dokument je roven 1.\n\nap_documents |&gt;\n  group_by(document) |&gt;\n  summarise(gamma_sum = sum(gamma))\n\n# A tibble: 2,246 Ã— 2\n   document gamma_sum\n   &lt;chr&gt;        &lt;dbl&gt;\n 1 1                1\n 2 10               1\n 3 100              1\n 4 1000             1\n 5 1001             1\n 6 1002             1\n 7 1003             1\n 8 1004             1\n 9 1005             1\n10 1006             1\n# â„¹ 2,236 more rows\n\n\nDokument 1 je skoro ÄistÃ© tÃ©ma Ä. 5:\n\nap_tidy |&gt;\n  filter(document == 1) |&gt;\n  arrange(desc(count))\n\n# A tibble: 186 Ã— 3\n   document term      count\n      &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt;\n 1        1 police        7\n 2        1 school        7\n 3        1 teacher       7\n 4        1 shot          5\n 5        1 students      5\n 6        1 boy           4\n 7        1 boys          4\n 8        1 classroom     4\n 9        1 gun           3\n10        1 guns          3\n# â„¹ 176 more rows\n\n\nAle dokument 2 je mix tÃ©mat 2 a 3. PojÄme se podÃ­vat blÃ­Å¾:\n\nap_tidy |&gt;\n  filter(document == 2) |&gt;\n  arrange(desc(count))\n\n# A tibble: 174 Ã— 3\n   document term      count\n      &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt;\n 1        2 peres        13\n 2        2 offer         9\n 3        2 official      8\n 4        2 bechtel       7\n 5        2 rappaport     7\n 6        2 israel        6\n 7        2 oil           6\n 8        2 memo          5\n 9        2 pipeline      5\n10        2 company       4\n# â„¹ 164 more rows\n\n\n\nlibrary(LDAvis)\n\n\npost &lt;- posterior(ap_lda)\nmat &lt;- ap_lda@wordassignments\n\nldavis_json &lt;- LDAvis::createJSON(\n  phi = post[[\"terms\"]],\n  theta = post[[\"topics\"]],\n  vocab = colnames(post[[\"terms\"]]),\n  doc.length = rowSums(as.matrix(mat), na.rm = TRUE),\n  term.frequency = colSums(as.matrix(mat), na.rm = TRUE)\n)\n\nif (interactive()) {\n  ldavis_json |&gt; serVis()\n}"
  },
  {
    "objectID": "analyses/ukazka_bertopic_r.html",
    "href": "analyses/ukazka_bertopic_r.html",
    "title": "UkÃ¡zka BERTopic",
    "section": "",
    "text": "NaÄteme R balÃ­ky.\nlibrary(tidyverse)\n\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” dplyr     1.1.4     âœ” readr     2.1.4\nâœ” forcats   1.0.0     âœ” stringr   1.5.0\nâœ” ggplot2   3.4.4     âœ” tibble    3.2.1\nâœ” lubridate 1.9.3     âœ” tidyr     1.3.0\nâœ” purrr     1.0.2     \nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::filter() masks stats::filter()\nâœ– dplyr::lag()    masks stats::lag()\nâ„¹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at /Users/netik/Documents/git/tmws\n\nlibrary(reticulate)\n\n# pomocnÃ© funkce\nsource(here(\"shared.R\"), local = TRUE)\nPouÅ¾ijeme prostÅ™edÃ­ Pythonu, kterÃ© jsme si nakonfigurovali dÅ™Ã­ve1.\nuse_condaenv(\"topic_modeling\")\nNaÄteme Python moduly.\nbt &lt;- import(\"bertopic\")\nbt_repre &lt;- import(\"bertopic.representation\")\nst &lt;- import(\"sentence_transformers\")\nsklearn_datasets &lt;- import(\"sklearn.datasets\")\ncv &lt;- import(\"sklearn.feature_extraction.text\")"
  },
  {
    "objectID": "analyses/ukazka_bertopic_r.html#data",
    "href": "analyses/ukazka_bertopic_r.html#data",
    "title": "UkÃ¡zka BERTopic",
    "section": "Data",
    "text": "Data\nKlasickÃ½ dataset 20 newsgroups je distribuovÃ¡n v modulu sklearn.datesets. Jde o cca 20K krÃ¡tkÃ½ch zprÃ¡v, kterÃ© si vymÄ›Åˆovali uÅ¾ivatelÃ© Usenetu (tj. nÄ›kdy v 90. letech). ZprÃ¡vy pochÃ¡zejÃ­ rovnomÄ›rnÄ› z 20 kanÃ¡lÅ¯.\nFunkce rovnou umoÅ¾Åˆuje smazat rÅ¯znÃ© technickÃ© ÄÃ¡sti zprÃ¡v, kterÃ© nÃ¡s nezajÃ­majÃ­. Nakonec z objektu vybereme pouze data.\n\nnewsgroups_raw &lt;- sklearn_datasets$fetch_20newsgroups(\n  subset = \"all\",\n  remove = tuple(\"headers\", \"footers\", \"quotes\")\n)\n\n# data samotnÃ¡ jsou jeÅ¡tÄ› o ÃºroveÅˆ nÃ­Å¾e\nd_newsgroups &lt;- newsgroups_raw[[\"data\"]]"
  },
  {
    "objectID": "analyses/ukazka_bertopic_r.html#trÃ©nujeme-model",
    "href": "analyses/ukazka_bertopic_r.html#trÃ©nujeme-model",
    "title": "UkÃ¡zka BERTopic",
    "section": "TrÃ©nujeme model",
    "text": "TrÃ©nujeme model\nZÃ­skÃ¡me zvlÃ¡Å¡Å¥ embeddings.\n\n# na Apple Silicon strojÃ­ch chceme vyuÅ¾Ã­t \"grafickou kartu\"\nst_device &lt;- NULL\n\nif (version$platform == \"aarch64-apple-darwin20\") {\n  st_device &lt;- \"mps\"\n}\n\n# definujeme model\nembedding_model &lt;- st$SentenceTransformer(\"all-MiniLM-L6-v2\", device = st_device)\n\n# spoÄÃ­tÃ¡me embeddings, pokud uÅ¾ je nemÃ¡me\nif (file.exists(here(\"data/embeddings/newsgroups.rds\"))) {\n  embeddings &lt;- read_rds(here(\"data/embeddings/newsgroups.rds\"))\n} else {\n  embeddings &lt;- embedding_model$encode(d_newsgroups, show_progress_bar = TRUE)\n}\n\nOdhadneme zbytek modelu.\n\ntopic_model &lt;- bt$BERTopic(language = \"english\", verbose = TRUE)\n\noutput &lt;- topic_model$fit_transform(d_newsgroups, embeddings = embeddings)\n\ntbl_output &lt;- tibble(\n  document = seq_along(d_newsgroups),\n  topic = output[[1]],\n  prob = output[[2]]\n)"
  },
  {
    "objectID": "analyses/ukazka_bertopic_r.html#vÃ½sledky",
    "href": "analyses/ukazka_bertopic_r.html#vÃ½sledky",
    "title": "UkÃ¡zka BERTopic",
    "section": "VÃ½sledky",
    "text": "VÃ½sledky\nZÃ¡kladnÃ­ pÅ™ehled tÃ©mat, poÄtÅ¯ pÅ™Ã­sluÅ¡nÃ½ch dokumentÅ¯ a shrnutÃ­ clusterÅ¯ zÃ­skÃ¡te skrze:\n\n# nÃ¡hled omezÃ­me jen na 5 prvnÃ­ch tÃ©mat a z plnÃ½ch ukÃ¡zek reprezentativnÃ­ch dokumentÅ¯\n# udÄ›lÃ¡me Ãºryvky o 100 znacÃ­ch (vÅ¡e kvÅ¯li renderovÃ¡nÃ­ reportu, pro prÃ¡ci v RStudiu\n# nenÃ­ tÅ™eba)\n\ntopic_model$get_topic_info() |&gt;\n  head() |&gt;\n  mutate(Representative_Docs = map(Representative_Docs, \\(x) str_trunc(x, 100)))\n\n\n  \n\n\n\nNejvÃ­ce zastoupenÃ© tÃ©ma je pod indexem 0. Podrobnosti o tÃ©matu zÃ­skÃ¡me pÅ™Ã­kazem nÃ­Å¾e. VÃ½stupem jsou slova s nejvyÅ¡Å¡Ã­ c-TF-IDF hodnotou, zobrazenou u kaÅ¾dÃ©ho slova. c-TF-IDF je TF-IDF fungujÃ­cÃ­ ne na Ãºrovni dokumentÅ¯, ale clusterÅ¯. Å˜Ã­kÃ¡, jakÃ¡ slova jsou nejvÃ­ce relevantnÃ­ pro danÃ© tÃ©ma; vychÃ¡zÃ­ ze â€œslepenceâ€ dokumentÅ¯ v rÃ¡mci clusteru.\n\ntopic_model$get_topic(4) |&gt; tuple_list_to_tibble()\n\n\n  \n\n\n\n\ntopic_model$visualize_barchart()\n\n\n\n\n\nUMAP do 2 dimenzÃ­\n\ntopic_model$visualize_documents(d_newsgroups, embeddings = embeddings, topics = tuple(as.list(1:10)))\n\n\n\n\n\n\ntopic_model$visualize_heatmap(top_n_topics = 50L)\n\n\n\n\n\n\ntopic_model$visualize_hierarchy(orientation = \"left\", top_n_topics = 10L)\n\nA marker object has been specified, but markers is not in the mode\nAdding markers to the mode...\nA marker object has been specified, but markers is not in the mode\nAdding markers to the mode...\nA marker object has been specified, but markers is not in the mode\nAdding markers to the mode...\nA marker object has been specified, but markers is not in the mode\nAdding markers to the mode...\nA marker object has been specified, but markers is not in the mode\nAdding markers to the mode...\nA marker object has been specified, but markers is not in the mode\nAdding markers to the mode...\nA marker object has been specified, but markers is not in the mode\nAdding markers to the mode...\nA marker object has been specified, but markers is not in the mode\nAdding markers to the mode...\nA marker object has been specified, but markers is not in the mode\nAdding markers to the mode...\n\n\n\n\n\n\nMÅ¯Å¾eme srovnat s nÄ›jakÃ½mi externÃ­mi informacemi. Tady napÅ™. vÃ­me, v jakÃ©m novinkovÃ©m kanÃ¡lu se zprÃ¡va objevila. MÅ¯Å¾eme pouÅ¾Ã­t i Ãºdaj o vÃ½zkumnÃ© skupinÄ› atp.\n\nclasses_nms &lt;- newsgroups_raw$target_names\nclasses_idx &lt;- newsgroups_raw$target + 1L # prevent zeros\n\n\nclasses_fct &lt;- factor(classes_idx, labels = classes_nms)\n\ntopics_per_class &lt;- topic_model$topics_per_class(d_newsgroups, classes_fct)\n\ntopic_model$visualize_topics_per_class(topics_per_class = topics_per_class)\n\n\n\n\n\nEmbeddings, UMAP a HDBSCAN nechÃ¡me, jak jsme je â€œnafitovaliâ€, ale zkusÃ­me poladit reprezentace clusterÅ¯.\nNejprve zkusÃ­me vyÅ™adit anglickÃ¡ stop slova, hledat a spojovat slovnÃ­ spojenÃ­ aÅ¾ o tÅ™ech slovech a brÃ¡t v potaz jen slova, co se vyskytnou alespoÅˆ 20krÃ¡t:\n\nvectorizer_model &lt;-  cv$CountVectorizer(stop_words=\"english\", ngram_range = tuple(1L, 3L), min_df = 20L)\n\ntopic_model$update_topics(d_newsgroups, vectorizer_model=vectorizer_model)\n\n\ntopic_model$visualize_barchart()\n\n\n\n\n\n\n# representation_model_keybert &lt;- bt_repre$KeyBERTInspired()\n# topic_model$update_topics(d_newsgroups, vectorizer_model=vectorizer_model, representation_model = representation_model_keybert)\n\nHrÃ¡t si mÅ¯Å¾eme i se samotnÃ½mi popisky. Tento krok se defaultnÄ› nepouÅ¾Ã­vÃ¡. ZkusÃ­me model od Facebooku, kterÃ½ umÃ­ obsah tÃ©mat klasifikovat do kategoriÃ­, na kterÃ½ch ani nemusel bÃ½t trÃ©novanÃ½ (tzv. â€œzero-shotâ€ klasifikace). Je ale tÅ™eba vymyslet sadu kandidÃ¡tnÃ­ch popiskÅ¯ (a doufat, Å¾e to nÃ¡Å¡ poÄÃ­taÄ upoÄÃ­tÃ¡).\n\ncandidate_topics &lt;- c(\"computers\", \"bicycles\", \"cars\", \"sport\", \"palestine\", \"health\")\nrepresentation_model &lt;- bt_repre$ZeroShotClassification(candidate_topics = candidate_topics, model=\"facebook/bart-large-mnli\") \n\n# updatujeme jenom repre model\ntopic_model$update_topics(d_newsgroups, representation_model = representation_model)\n\n\ntopic_model$get_topic_info() \ntopic_model$visualize_barchart()\n\nPozn: mÃ¡me-li penÃ­ze a API na OpenAI, mÅ¯Å¾eme o shrnutÃ­ tÃ©mat poprosit ChatGPT."
  },
  {
    "objectID": "prezi/index.html#section",
    "href": "prezi/index.html#section",
    "title": "Topic modeling ğŸ“š",
    "section": "",
    "text": "Topicâ€¦ co? ğŸ¤”"
  },
  {
    "objectID": "prezi/index.html#topic-modeling",
    "href": "prezi/index.html#topic-modeling",
    "title": "Topic modeling ğŸ“š",
    "section": "Topic modeling",
    "text": "Topic modeling\n\n\nvelkÃ© mnoÅ¾stvÃ­ textu\nrozdÄ›lenÃ­ do dokumentÅ¯\nâ“ jakÃ¡ tÃ©mata se v textu objevujÃ­\nâ“ jakÃ© je zastoupenÃ­ tÃ©mat v jednotlivÃ½ch dokumentech\n\n\n\n\n\n\n\n\n\nPozor\n\n\nTM vs.Â shrnutÃ­ textu, extrakce klÃ­ÄovÃ½ch slov, zero-shot klasifikace, rozeznÃ¡vÃ¡nÃ­ pojmenovanÃ½ch entit atp.\n\n\n\n\n\n\noblast natural language processing (NLP), nÄ›kdo Å™Ã­kÃ¡ text analysis\nsupervidovanÃ© vs.Â nesupervidovanÃ© (TM, clusteringâ€¦) uÄenÃ­"
  },
  {
    "objectID": "prezi/index.html#nÄ›co-na-ÄtenÃ­",
    "href": "prezi/index.html#nÄ›co-na-ÄtenÃ­",
    "title": "Topic modeling ğŸ“š",
    "section": "NÄ›co na ÄtenÃ­",
    "text": "NÄ›co na ÄtenÃ­\n\n\n\n\n\n\n\nSilge & Robinson (2017)\n\n\n\n\n\n\nHvitfeldt & Silge (2021)"
  },
  {
    "objectID": "prezi/index.html#z-historie",
    "href": "prezi/index.html#z-historie",
    "title": "Topic modeling ğŸ“š",
    "section": "Z â€œhistorieâ€",
    "text": "Z â€œhistorieâ€\n\npreprocessing, stopwords, tokenizace (viz Hvitfeldt & Silge, 2021)\nco s flektivnÃ­mi jazyky jako ÄeÅ¡tina?\n\nlemmatizace, stemming\n\ntokeny s sebou nenesou kontext\n(lze trochu obejÃ­t pomocÃ­ n-gramÅ¯)"
  },
  {
    "objectID": "prezi/index.html#n-gramy",
    "href": "prezi/index.html#n-gramy",
    "title": "Topic modeling ğŸ“š",
    "section": "N-gramy",
    "text": "N-gramy\n\nmÃ­sto topic a modeling zavedu topic modeling\npohyblivÃ© okno nad textem â†’ moÅ¾nost grafÅ¯"
  },
  {
    "objectID": "prezi/index.html#n-gramy-1",
    "href": "prezi/index.html#n-gramy-1",
    "title": "Topic modeling ğŸ“š",
    "section": "N-gramy",
    "text": "N-gramy\n\n\nSilge & Robinson (2017)"
  },
  {
    "objectID": "prezi/index.html#z-historie-2",
    "href": "prezi/index.html#z-historie-2",
    "title": "Topic modeling ğŸ“š",
    "section": "Z â€œhistorieâ€ 2",
    "text": "Z â€œhistorieâ€ 2\n\nÄetnosti (term frequency)\nTF-IDF (term frequency â€“ inverse document frequency; souÄin)\nIDF(\\text{token}) = \\ln{\\left(\\frac{n_{\\text{dokumentÅ¯}}}{n_{\\text{dokumentÅ¯ s tokenem}}}\\right)}\ndocument-term matrix (DTM)"
  },
  {
    "objectID": "prezi/index.html#section-1",
    "href": "prezi/index.html#section-1",
    "title": "Topic modeling ğŸ“š",
    "section": "",
    "text": "Document-term matrix\n\n\n\ntopic\nmodeling\nâ€¦\n\n\n\n\ndokument Ä. 1\n0\n5\nâ€¦\n\n\ndokument Ä. 2\n2\n1\nâ€¦\n\n\ndokument Ä. 3\n4\n8\nâ€¦\n\n\nâ€¦\nâ€¦\nâ€¦\nâ€¦"
  },
  {
    "objectID": "prezi/index.html#pÅ™Ã­stupy-k-topic-modelingu",
    "href": "prezi/index.html#pÅ™Ã­stupy-k-topic-modelingu",
    "title": "Topic modeling ğŸ“š",
    "section": "PÅ™Ã­stupy k topic modelingu",
    "text": "PÅ™Ã­stupy k topic modelingu\n\nlatentnÃ­ Dirichletova alokace (Latent Dirchlet Allocation, LDA, Blei et al., 2003)\nNon-negative Matrix Factorization (NMF)\nLatent Semantic Analysis (LSA)"
  },
  {
    "objectID": "prezi/index.html#latent-dirchlet-allocation",
    "href": "prezi/index.html#latent-dirchlet-allocation",
    "title": "Topic modeling ğŸ“š",
    "section": "Latent Dirchlet Allocation",
    "text": "Latent Dirchlet Allocation\n\nkaÅ¾dÃ½ dokument je smÄ›sicÃ­ rÅ¯znÃ½ch tÃ©mat\nkaÅ¾dÃ© tÃ©ma se sklÃ¡dÃ¡ z mixu slov\njednotlivÃ¡ slova mohou bÃ½t zastoupena ve vÃ­ce tÃ©matech"
  },
  {
    "objectID": "prezi/index.html#problÃ©my-tradiÄnÃ­ch-pÅ™Ã­stupÅ¯",
    "href": "prezi/index.html#problÃ©my-tradiÄnÃ­ch-pÅ™Ã­stupÅ¯",
    "title": "Topic modeling ğŸ“š",
    "section": "ProblÃ©my tradiÄnÃ­ch pÅ™Ã­stupÅ¯",
    "text": "ProblÃ©my tradiÄnÃ­ch pÅ™Ã­stupÅ¯\n\nnutnost preprocessingu\ntokeny standardnÄ› bez kontextu\nnÃ¡roÄnÃ¡ interpretace vÃ½sledkÅ¯"
  },
  {
    "objectID": "prezi/index.html#moÅ¾nosti",
    "href": "prezi/index.html#moÅ¾nosti",
    "title": "Topic modeling ğŸ“š",
    "section": "MoÅ¾nosti",
    "text": "MoÅ¾nosti\n\nword embeddings (â€œvnoÅ™enÃ­ slovâ€)\n\npÅ™evedenÃ­ textovÃ½ch dat na ÄÃ­sla\nÄÃ­sla = N-dimenzionÃ¡lnÃ­ vektory v prostoru\nmÅ¯Å¾eme dÄ›lat matematiku, napÅ™:\n\nkosinovÃ¡ podobnost (cos Ãºhlu mezi 2 vektory) nebo\nvec(\\text{krÃ¡l}) - vec(\\text{muÅ¾}) + vec(\\text{Å¾ena}) = vec(\\text{krÃ¡lovna})\n\n\n\n\n\nword2vec (Mikolov et al., 2013)"
  },
  {
    "objectID": "prezi/index.html#moÅ¾nosti-2",
    "href": "prezi/index.html#moÅ¾nosti-2",
    "title": "Topic modeling ğŸ“š",
    "section": "MoÅ¾nosti 2",
    "text": "MoÅ¾nosti 2\n\n\nprÅ¯mÄ›r word embeddings za kaÅ¾dÃ½ dokument\nredukce dimenzionality\nclustering\nâ“â“â“"
  },
  {
    "objectID": "prezi/index.html#moÅ¾nosti-3",
    "href": "prezi/index.html#moÅ¾nosti-3",
    "title": "Topic modeling ğŸ“š",
    "section": "MoÅ¾nosti 3",
    "text": "MoÅ¾nosti 3\n\ntop2vec (Angelov, 2020)\n\nembeddings pro dokumenty a slova ve stejnÃ©m prostoru\nredukce dimenzionality\nclustering\nhledÃ¡nÃ­ nejbliÅ¾Å¡Ã­ch slov kolem centroidÅ¯ clusterÅ¯ â†’ interpretace"
  },
  {
    "objectID": "prezi/index.html#moÅ¾nosti-4",
    "href": "prezi/index.html#moÅ¾nosti-4",
    "title": "Topic modeling ğŸ“š",
    "section": "MoÅ¾nosti 4",
    "text": "MoÅ¾nosti 4\n\nBERTopic (Grootendorst, 2022)\n\nembeddings dokumentÅ¯\nredukce dimenzionality\nclustering\nslouÄenÃ­ obsahu dokumentÅ¯ v rÃ¡mci clusterÅ¯, tokenizace\nreprezentace tÃ©mat pomocÃ­ upravenÃ© TF-IDF1\n\n\n\n\nDokumentace na maartengr.github.io/BERTopic\nâ€œc-TF-IDFâ€: jakÃ¡ slova jsou nejvÃ­ce relevantnÃ­ pro danÃ© tÃ©ma."
  },
  {
    "objectID": "prezi/index.html#section-2",
    "href": "prezi/index.html#section-2",
    "title": "Topic modeling ğŸ“š",
    "section": "",
    "text": "Zdroj: maartengr.github.io/BERTopic"
  },
  {
    "objectID": "prezi/index.html#bertopic-embeddings",
    "href": "prezi/index.html#bertopic-embeddings",
    "title": "Topic modeling ğŸ“š",
    "section": "BERTopic â€“ embeddings",
    "text": "BERTopic â€“ embeddings\n\nhlavnÃ­ â€œobjektâ€ celÃ©ho modelovÃ¡nÃ­\npÅ™edtrÃ©novanÃ½ model SBERT (Reimers & Gurevych, 2019)\n\nspeciÃ¡lnÄ› uzpÅ¯sobenÃ½ na vÄ›ty (a kratÅ¡Ã­ odstavce)1\n\n\nDelÅ¡Ã­ celky je vhodnÃ© rozdÄ›lit, nebo pouÅ¾Ã­t jinÃ½ model."
  },
  {
    "objectID": "prezi/index.html#bertopic-redukce-dimenzÃ­",
    "href": "prezi/index.html#bertopic-redukce-dimenzÃ­",
    "title": "Topic modeling ğŸ“š",
    "section": "BERTopic â€“ redukce dimenzÃ­",
    "text": "BERTopic â€“ redukce dimenzÃ­\n\nmÃ¡me data s 18 846 rÃ¡dky 384 sloupci (384dimenzionÃ¡lnÃ­ embeddings)\npotÅ™eba redukce\nmetoda UMAP (Uniform Manifold Approximation and Projection for Dimension Reduction)\npozor â€“ stochastickÃ©"
  },
  {
    "objectID": "prezi/index.html#bertopic-clustering",
    "href": "prezi/index.html#bertopic-clustering",
    "title": "Topic modeling ğŸ“š",
    "section": "BERTopic â€“ clustering",
    "text": "BERTopic â€“ clustering\n\nidentifikace skupin mezi dokumenty\nmetoda HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise)\nhustota prvkÅ¯ v prostoru\nmoÅ¾nost outlierÅ¯"
  },
  {
    "objectID": "prezi/index.html#section-3",
    "href": "prezi/index.html#section-3",
    "title": "Topic modeling ğŸ“š",
    "section": "",
    "text": "Zdroj: scikit-learn.org/stable/modules/clustering.html"
  },
  {
    "objectID": "prezi/index.html#reference",
    "href": "prezi/index.html#reference",
    "title": "Topic modeling ğŸ“š",
    "section": "Reference",
    "text": "Reference\n\n\n\n\n\n\n\n\n\nAngelov, D. (2020). Top2Vec: Distributed Representations of Topics. arXiv. https://arxiv.org/abs/2008.09470\n\n\nBlei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet Allocation. The Journal of Machine Learning Research, 3, 993â€“1022.\n\n\nGrootendorst, M. (2022). BERTopic: Neural topic modeling with a class-based TF-IDF procedure. arXiv preprint arXiv:2203.05794.\n\n\nHvitfeldt, E., & Silge, J. (2021). Supervised Machine Learning for Text Analysis in R. https://doi.org/10.1201/9781003093459\n\n\nMikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. arXiv. https://arxiv.org/abs/1301.3781\n\n\nReimers, N., & Gurevych, I. (2019). Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. https://arxiv.org/abs/1908.10084\n\n\nSilge, J., & Robinson, D. (2017). Text Mining with R: A Tidy Approach. Oâ€™Reilly Media."
  }
]