---
title: Topic modeling üìö
subtitle: Jak vytƒõ≈æit jazykov√© modely a nep≈ôij√≠t o pr√°ci
author:
  - name: Jan Net√≠k
    email: netikja@gmail.com
    orcid: 0000-0002-3888-3203
    affiliation: √öI AV ƒåR / Schola Empirica / PedF UK
date: 2023-11-30

format:
  revealjs:
    title-slide-attributes: 
      data-background-color: "#0066cc"
    html-math-method: katex
    
date-format: medium
df-print: kable
lang: cs
bibliography: references.bib
csl: apa.csl
---

```{r}
library(tidyverse)
```

##  {.center}

::: {style="font-size:2.5em;text-align:center"}
Topic[...]{.fragment} [co? ü§î]{.fragment}
:::

## Topic modeling

::: incremental
-   velk√© mno≈æstv√≠ textu

-   rozdƒõlen√≠ do **dokument≈Ø**

-   ‚ùì jak√° t√©mata se v textu objevuj√≠

-   ‚ùì jak√© je zastoupen√≠ t√©mat v jednotliv√Ωch dokumentech
:::

::: {.callout-warning .fragment}
## Pozor

TM vs. shrnut√≠ textu, extrakce kl√≠ƒçov√Ωch slov, zero-shot klasifikace, rozezn√°v√°n√≠ pojmenovan√Ωch entit atp.
:::

::: notes
-   oblast natural language processing (NLP), nƒõkdo ≈ô√≠k√° text analysis
-   supervidovan√© vs. **nesupervidovan√© (TM, clustering...)** uƒçen√≠
:::

## Nƒõco na ƒçten√≠ {.smaller}

::: columns
::: {.column style="text-align: center"}
![](figs/text_mining_cover.png){fig-align="center" height="500"}

@silge2017
:::

::: {.column style="text-align: center"}
![](figs/smltar_cover.jpg){fig-align="center" height="500"}

@hvitfeldt2021
:::
:::

## Z "historie"

-   preprocessing, stopwords, tokenizace [viz @hvitfeldt2021]
-   co s flektivn√≠mi jazyky jako ƒçe≈°tina?
    -   lemmatizace, stemming
-   **tokeny** s sebou nenesou kontext\
    (lze trochu obej√≠t pomoc√≠ *n*-gram≈Ø)

## *N*-gramy

-   m√≠sto `topic` a `modeling` zavedu `topic modeling`
-   pohybliv√© okno nad textem ‚Üí mo≈ænost graf≈Ø

## *N*-gramy {.smaller}

![](figs/bigrams_bible.png){.r-stretch fig-align="center"}

::: {style="text-align: right"}
@silge2017
:::

## Z "historie" 2

-   ƒçetnosti (term frequency)
-   **TF-IDF** (term frequency -- inverse document frequency; *souƒçin*)\
    $$IDF(\text{token}) = \ln{\left(\frac{n_{\text{dokument≈Ø}}}{n_{\text{dokument≈Ø s tokenem}}}\right)}$$
-   document-term matrix (DTM)

## 

|                   | topic | modeling | ... |
|-------------------|-------|----------|-----|
| **dokument ƒç. 1** | 0     | 5        | ... |
| **dokument ƒç. 2** | 2     | 1        | ... |
| **dokument ƒç. 3** | 4     | 8        | ... |
| ...               | ...   | ...      | ... |

: Document-term matrix

## P≈ô√≠stupy k topic modelingu

-   latentn√≠ Dirichletova alokace [Latent Dirchlet Allocation, LDA, @lda]
-   [Non-negative Matrix Factorization (NMF)]{style="color:#6f6f6f"}
-   [Latent Semantic Analysis (LSA)]{style="color:#6f6f6f"}

## Latent Dirchlet Allocation

-   ka≈æd√Ω dokument je smƒõsic√≠ r≈Øzn√Ωch t√©mat
-   ka≈æd√© t√©ma se skl√°d√° z mixu slov
-   jednotliv√° slova mohou b√Ωt zastoupena ve v√≠ce t√©matech

# Uk√°zka 1 {background-color="#0066cc"}

Latent Dirchlet Allocation

## Probl√©my tradiƒçn√≠ch p≈ô√≠stup≈Ø

-   nutnost preprocessingu
-   tokeny standardnƒõ bez kontextu
-   n√°roƒçn√° interpretace v√Ωsledk≈Ø

## Mo≈ænosti

-   word **embeddings** ("vno≈ôen√≠ slov")

    -   p≈ôeveden√≠ textov√Ωch dat na ƒç√≠sla
    -   ƒç√≠sla = N-dimenzion√°ln√≠ vektory v prostoru
    -   m≈Ø≈æeme dƒõlat matematiku, nap≈ô:
        -   kosinov√° podobnost ($cos$ √∫hlu mezi 2 vektory) nebo
        -   $vec(\text{kr√°l}) - vec(\text{mu≈æ}) + vec(\text{≈æena}) =$ [$vec(\text{kr√°lovna})$]{.fragment}

    ::: aside
    `word2vec` [@mikolov2013]
    :::

------------------------------------------------------------------------

{{< video https://www.youtube.com/watch?v=viZrOnJclY0 width="100%" height="100%" >}}

## Mo≈ænosti 2 {auto-animate="true"}

::: incremental
1.  pr≈Ømƒõr word embeddings za ka≈æd√Ω dokument

2.  redukce dimenzionality

3.  clustering

4.  ‚ùì‚ùì‚ùì
:::

## Mo≈ænosti 3 {auto-animate="true"}

-   `top2vec` [@angelov2020]
    1.  embeddings pro dokumenty a slova ve stejn√©m prostoru
    2.  redukce dimenzionality
    3.  clustering
    4.  hled√°n√≠ nejbli≈æ≈°√≠ch slov kolem centroid≈Ø cluster≈Ø ‚Üí interpretace

## Mo≈ænosti 4 {auto-animate="true"}

-   `BERTopic` [@grootendorst2022]
    1.  embeddings dokument≈Ø
    2.  redukce dimenzionality
    3.  clustering
    4.  slouƒçen√≠ obsahu dokument≈Ø v r√°mci cluster≈Ø, tokenizace
    5.  reprezentace t√©mat pomoc√≠ upraven√© TF-IDF

::: aside
Dokumentace na [maartengr.github.io/BERTopic](https://maartengr.github.io/BERTopic/)
:::

##  {.smaller}

![](figs/bertopic_algorithm.svg){.r-stretch fig-align="left"}

::: {style="text-align: right"}
Zdroj: [maartengr.github.io/BERTopic](https://maartengr.github.io/BERTopic/)
:::

## BERTopic -- embeddings

-   hlavn√≠ "objekt" cel√©ho modelov√°n√≠
-   p≈ôedtr√©novan√Ω model SBERT [@reimers2019sentencebert]
    -   speci√°lnƒõ uzp≈Øsoben√Ω na vƒõty (a krat≈°√≠ odstavce)[^1]

[^1]: Del≈°√≠ celky je vhodn√© rozdƒõlit, nebo pou≈æ√≠t jin√Ω model.

## BERTopic -- redukce dimenz√≠

-   m√°me data s 18 846 r√°dky 384 sloupci (384dimenzion√°ln√≠ embeddings)
-   pot≈ôeba redukce
-   metoda UMAP (Uniform Manifold Approximation and Projection for Dimension Reduction)

## BERTopic -- clustering

-   identifikace skupin mezi dokumenty

# Uk√°zka 2 {background-color="#0066cc"}

BERTopic

## Reference

::: refs
:::
